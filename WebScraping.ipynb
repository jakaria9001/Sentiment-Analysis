{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"WebScraping.ipynb","provenance":[],"authorship_tag":"ABX9TyN5RSjuqwZFxHzNee7aCpIQ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0-SDogzZLDCV"},"source":["#Web Scrapping using Scrapy.\n","\n","In This Notebook we will be doing web scrapping from the website <a href = \"superdatascience.com/scrapy_practical\">SuperDataScience</a>.  \n","\n","*Definition :*\n","\n","Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites. Web scraping software may access the World Wide Web directly using the Hypertext Transfer Protocol, or through a web browser."]},{"cell_type":"code","metadata":{"id":"YQ-njIAQS2Ff"},"source":["# pip show scrapy\n","# !pip install scrapy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cLEPDa85SqEQ"},"source":["**About the functions used :** \n","\n","name: identifies the Spider. It must be unique within a project, that is, you canâ€™t set the same name for different Spiders.\n","\n","start_requests(): must return an iterable of Requests (you can return a list of requests or write a generator function) which the Spider will begin to crawl from. Subsequent requests will be generated successively from these initial requests.\n","\n","parse(): a method that will be called to handle the response downloaded for each of the requests made. The response parameter is an instance of TextResponse that holds the page content and has further helpful methods to handle it.\n","\n","The parse() method usually parses the response, extracting the scraped data as dicts and also finding new URLs to follow and creating new requests (Request) from them."]},{"cell_type":"code","metadata":{"id":"WEyD-IcVKf80"},"source":["# Importing libraries :\n","import scrapy\n","from scrapy.http import Response\n","\n","# Creating Class:\n","class TestSpider(scrapy.Spider):\n","  name = 'practical_spider'\n","  allowed_domains = ['superdatascience.com']\n","  start_urls = ['https://superdatascience.com/scrapy_practical/']\n","  \n","  def parse(self, response):\n","    item = {\n","        'logo': response.css('#header > div > div > a > img').xpath('@src').extract_first(),\n","        'q1': response.css('#content > div.article-content > strong::text').extract_first(),\n","        'q11': response.css('#content > div.article-content > h2:nth-child(7)::text').extract_first(),\n","        'q2': response.css('#content > div.nth-child(2)> h2::text').extract_first(),\n","        'list_items':[\n","                      e for e in\n","                      response.css('#content > div.article-content > ul *::text').extract() if e.strip()\n","        ],\n","\n","\n","    }\n","    self.logger.info('aaa %s',item)\n","    return item\n"],"execution_count":null,"outputs":[]}]}